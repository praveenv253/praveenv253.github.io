<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://praveenv253.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://praveenv253.github.io/" rel="alternate" type="text/html" /><updated>2023-04-14T19:54:46-07:00</updated><id>https://praveenv253.github.io/feed.xml</id><title type="html">Praveen’s website</title><subtitle>Praveen Venkatesh's personal website.</subtitle><author><name>Praveen Venkatesh</name></author><entry><title type="html">Defining and Inferring Flows of Information in Neural Circuits</title><link href="https://praveenv253.github.io/research/2020/08/05/info-flow-in-comp-sys.html" rel="alternate" type="text/html" title="Defining and Inferring Flows of Information in Neural Circuits" /><published>2020-08-05T16:06:00-07:00</published><updated>2020-08-05T16:06:00-07:00</updated><id>https://praveenv253.github.io/research/2020/08/05/info-flow-in-comp-sys</id><content type="html" xml:base="https://praveenv253.github.io/research/2020/08/05/info-flow-in-comp-sys.html">&lt;p&gt;While studying existing methods for inferring information flow in neuroscience, we recognized that Granger Causality &lt;a href=&quot;/publications#Venkatesh2015Direction&quot;&gt;does not always&lt;/a&gt; capture information flow &lt;em&gt;about a specific message or stimulus&lt;/em&gt;. We were able to construct a counterexample wherein Granger Causality inferred the incorrect flow of a message, &lt;em&gt;even when all nodes were observed and when there was no measurement noise&lt;/em&gt;. This counterexample also applied to generalizations of Granger Causality, such as Transfer Entropy and Directed Information. We believe that an important reason for the failure of Granger Causality-based tools in this context was the lack of a &lt;em&gt;formal definition for information flow&lt;/em&gt; pertaining to a specific &lt;em&gt;message&lt;/em&gt; (which could be the stimulus or response in a neuroscientific experiment). The development of such a definition, in turn, was impeded by the absence of a concrete theoretical framework that linked information flow about a message to empirical measurements. &lt;a href=&quot;/publications#Venkatesh2018Information&quot;&gt;We addressed this fundamental gap&lt;/a&gt; in our understanding by proposing a new computational model of the brain, and by providing a new definition for information flow about a message within this model.&lt;/p&gt;

&lt;p&gt;A central contribution of our work lies in recognizing that tracking the flow of information about a specific message &lt;em&gt;requires&lt;/em&gt; that we also account for &lt;em&gt;synergistic&lt;/em&gt; flows of information. To understand why, note that a message $M$ may be represented, in combination with some independent “noise” variable $Z$, as $M{+}Z$ and $Z$, across two different brain areas. Represented in this form, if the noise $Z$ is large, it can be very hard to ascertain the presence of information flow about $M$ in the combination $[M{+}Z, Z]$, unless we account for possible synergy between these two brain areas. Accounting for synergy, using conditional mutual information for instance, is thus crucial for tracking information flow in arbitrary computational systems. Relatedly, I have also begun exploring how synergy, along with other partial information measures such as uniqueness and redundancy, arise and interact in neural encoding using entorhinal grid cells as a case study (&lt;a href=&quot;/publications#Venkatesh2020Understanding&quot;&gt;Venkatesh and Grover, &lt;em&gt;Cosyne&lt;/em&gt;, 2020&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/xor-counterexample.png&quot; alt=&quot;Cannot track information flow without accounting for synergy&quot; width=&quot;400px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our new measure of information flow does not have the same pitfalls as Granger Causality, while satisfying our intuition in several canonical examples of computational systems. The full impact of this work continues to be realized, as &lt;a href=&quot;/publications#Venkatesh2020Define&quot;&gt;we explore new measures in greater depth&lt;/a&gt;, demonstrate and validate these methods on artificial neural networks, and examine the causal implications of making interventions based on information flow inferences.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/publications#Venkatesh2018Information&quot; class=&quot;btn btn--light&quot;&gt;&lt;i class=&quot;fa fa-file-text-o fa-lg&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Main paper in Transactions on Info Theory&lt;/a&gt;
&lt;a href=&quot;/publications#Venkatesh2015Direction_Allerton&quot; class=&quot;btn btn--light&quot;&gt;&lt;i class=&quot;fa fa-file-text-o fa-lg&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; 2015 Granger Causality Counterexample paper&lt;/a&gt;
&lt;a href=&quot;/publications#Venkatesh2020Define&quot; class=&quot;btn btn--light&quot;&gt;&lt;i class=&quot;fa fa-file-text-o fa-lg&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; 2020 ISIT paper on alternative definitions&lt;/a&gt;&lt;/p&gt;</content><author><name>Praveen Venkatesh</name></author><category term="research" /><category term="Information flow" /><category term="Message" /><category term="Computational system" /><category term="Granger Causality" /><category term="Synergy" /><category term="Partial information decomposition" /><summary type="html">While studying existing methods for inferring information flow in neuroscience, we recognized that Granger Causality does not always capture information flow about a specific message or stimulus. We were able to construct a counterexample wherein Granger Causality inferred the incorrect flow of a message, even when all nodes were observed and when there was no measurement noise. This counterexample also applied to generalizations of Granger Causality, such as Transfer Entropy and Directed Information. We believe that an important reason for the failure of Granger Causality-based tools in this context was the lack of a formal definition for information flow pertaining to a specific message (which could be the stimulus or response in a neuroscientific experiment). The development of such a definition, in turn, was impeded by the absence of a concrete theoretical framework that linked information flow about a message to empirical measurements. We addressed this fundamental gap in our understanding by proposing a new computational model of the brain, and by providing a new definition for information flow about a message within this model.</summary></entry><entry><title type="html">Encoding in Grid Cells: Uniqueness, Redundancy and Synergy</title><link href="https://praveenv253.github.io/research/2020/08/05/pid-grid-cells-cosyne.html" rel="alternate" type="text/html" title="Encoding in Grid Cells: Uniqueness, Redundancy and Synergy" /><published>2020-08-05T16:06:00-07:00</published><updated>2020-08-05T16:06:00-07:00</updated><id>https://praveenv253.github.io/research/2020/08/05/pid-grid-cells-cosyne</id><content type="html" xml:base="https://praveenv253.github.io/research/2020/08/05/pid-grid-cells-cosyne.html">&lt;p&gt;An important contribution of my earlier work on &lt;a href=&quot;/publications#Venkatesh2018Information&quot;&gt;information flow&lt;/a&gt; lies in recognizing that accounting for synergy is essential if we want a measure of information flow that can guarantee the ability to track a message as it flows through a computational system. I presented a &lt;a href=&quot;/publications#Venkatesh2020Understanding&quot;&gt;poster at Cosyne&lt;/a&gt; this year, which supports and extends my information flow result in several key directions:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;First, it demonstrates how ideas such as uniqueness, redundancy and synergy are highly relevant in neural systems, through a case study on entorhinal grid cells;&lt;/li&gt;
  &lt;li&gt;Second, it shows how recent technical advances in information theory provide new ways to &lt;em&gt;quantify&lt;/em&gt; uniqueness, redundancy and synergy in such systems, giving neuroscientists a new tool, and the ability to pose and answer experimental questions in a completely new way (e.g., how much of the information about an animal’s location is redundantly encoded between grid cells and place cells?);&lt;/li&gt;
  &lt;li&gt;Thirdly, it shows that synergy can arise in unexpected ways—indeed, synergy sometimes appears when we ask a different &lt;em&gt;question&lt;/em&gt;. We show that each grid module encodes &lt;em&gt;unique&lt;/em&gt; information about an animal’s &lt;em&gt;precise&lt;/em&gt; location; however, when we ask how these modules encode information about the animal’s location at a very &lt;em&gt;coarse&lt;/em&gt; spatial scale, the same modules encode coarse location information &lt;em&gt;synergistically&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This work thus highlights how synergy may be common, and that it depends on how one looks at the same information. I chose to highlight this work in spite of it being an abstract because it fits well into my future vision: I seek to discover, through collaborations, more examples of neural systems that are ripe for information-theoretic exploration.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/doc/abstracts/2020--cosyne--abstract.pdf&quot; class=&quot;btn btn--light&quot;&gt;&lt;i class=&quot;fa fa-file-text-o fa-lg&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Cosyne 2020 Abstract&lt;/a&gt;
&lt;a href=&quot;/assets/doc/posters/2020--cosyne--poster.pdf&quot; class=&quot;btn btn--light&quot;&gt;&lt;i class=&quot;fa fa-file-text-o fa-lg&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Cosyne 2020 Poster&lt;/a&gt;&lt;/p&gt;</content><author><name>Praveen Venkatesh</name></author><category term="research" /><category term="Grid cells" /><category term="Partial information decomposition" /><category term="Synergy" /><summary type="html">An important contribution of my earlier work on information flow lies in recognizing that accounting for synergy is essential if we want a measure of information flow that can guarantee the ability to track a message as it flows through a computational system. I presented a poster at Cosyne this year, which supports and extends my information flow result in several key directions:</summary></entry><entry><title type="html">Systematically questioning the self-fulfilling prophecy of EEG’s low resolution</title><link href="https://praveenv253.github.io/research/2017/08/12/eeg-self-fulfilling-prophecy.html" rel="alternate" type="text/html" title="Systematically questioning the self-fulfilling prophecy of EEG’s low resolution" /><published>2017-08-12T09:01:00-07:00</published><updated>2017-08-12T09:01:00-07:00</updated><id>https://praveenv253.github.io/research/2017/08/12/eeg-self-fulfilling-prophecy</id><content type="html" xml:base="https://praveenv253.github.io/research/2017/08/12/eeg-self-fulfilling-prophecy.html">&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Electroencephalography&quot;&gt;Electroencephalography&lt;/a&gt;
(EEG) has long been believed to be a low-resolution brain imaging modality.
There is a pervasive view in the community of EEG users that increasing the
number of EEG electrodes will not improve imaging resolution. We have worked
towards systematically understanding the reasons this view exists, and towards
changing perceptions about EEG.&lt;/p&gt;

&lt;p&gt;The heart of our argument is explained in our &lt;a href=&quot;/publications#Grover2017Information&quot;&gt;recent
paper&lt;/a&gt; titled “An Information-theoretic
View of EEG Sensing”. We examined previous estimates of the number of sensors
needed to recover the EEG signal. These estimates relied on computing the
“spatial Nyquist rate” of scalp EEG, and it turned out that they underestimated
the number of sensors required to reliably recover the &lt;em&gt;continuous scalp
potential&lt;/em&gt;. We also showed, through simulations and intuitive arguments, that
the number of EEG sensors required to recover the &lt;em&gt;signal within the brain&lt;/em&gt;
could be even higher than the number prescribed by the spatial Nyquist rate.&lt;/p&gt;

&lt;p&gt;Naturally, the follow-up question is: “What are the fundamental theoretical
limits of the imaging resolution achievable by EEG, and how does this
resolution improve with increasing numbers of sensors?” While this paper gives
a first-pass at understanding these limits, we followed up with &lt;a href=&quot;/publications#Venkatesh2017Lower&quot;&gt;a
paper&lt;/a&gt; that gave the first analytical results
showing that EEG’s imaging resolution could improve with an increase in the
number of EEG sensors.&lt;/p&gt;

&lt;p&gt;We also gave information-theoretic techniques to help save power in a future
implementation of an ultra-high-density EEG system. This last point, which
focuses on the practical aspects of instrumenting such a high-density EEG
system was first discussed in &lt;a href=&quot;/publications#Grover2015Information&quot;&gt;Allerton
2015&lt;/a&gt;, where we introduced a “hierarchical
referencing mechanism” to save power and circuit area.&lt;/p&gt;

&lt;p&gt;Towards showing experimentally and practically that high density EEG, and
specifically, super-Nyquist sampling is more informative than conventional EEG,
we recently conducted some experiments in collaboration with Amanda Robinson,
Marlene Behrmann and Mike Tarr at CMU’s Psychology department, which &lt;a href=&quot;/publications#Robinson2017Very&quot;&gt;show
that&lt;/a&gt; high-density EEG has better
classification accuracy than low-density EEG in classifying between visual
stimuli of different spatial frequencies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/publications#Grover2017Information&quot; class=&quot;btn btn--light&quot;&gt;&lt;i class=&quot;fa fa-file-text-o fa-lg&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Link to paper&lt;/a&gt;
&lt;a href=&quot;/publications#Venkatesh2017High&quot; class=&quot;btn btn--light&quot;&gt;&lt;i class=&quot;fa fa-file-text-o fa-lg&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Link to poster&lt;/a&gt;&lt;/p&gt;</content><author><name>Praveen Venkatesh</name></author><category term="research" /><category term="EEG" /><category term="Nyquist rate" /><category term="Fundamental limits" /><category term="Experimental validation" /><summary type="html">Electroencephalography (EEG) has long been believed to be a low-resolution brain imaging modality. There is a pervasive view in the community of EEG users that increasing the number of EEG electrodes will not improve imaging resolution. We have worked towards systematically understanding the reasons this view exists, and towards changing perceptions about EEG.</summary></entry><entry><title type="html">Directions of information flow and Granger Causality</title><link href="https://praveenv253.github.io/research/2017/08/11/gc-info-flows.html" rel="alternate" type="text/html" title="Directions of information flow and Granger Causality" /><published>2017-08-11T18:06:00-07:00</published><updated>2017-08-11T18:06:00-07:00</updated><id>https://praveenv253.github.io/research/2017/08/11/gc-info-flows</id><content type="html" xml:base="https://praveenv253.github.io/research/2017/08/11/gc-info-flows.html">&lt;p&gt;Granger causality is an established measure of the “causal influence” that
one statistical process has on another. It has been used extensively in
neuroscience to infer statistical causal influences. Recently, however,
many works in the neuroscience literature have begun to compare Granger
causal influences along forward and reverse links of a feedback network in
order to determine the direction of information flow in this network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/gc-vs-message.jpg&quot; alt=&quot;Greater GC can be opposite the direction of Info flow&quot; width=&quot;400px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We asked whether comparing Granger causal influences correctly captures the
direction of information flow in a simple feedback network. We discovered,
using simple theoretical experiments, that comparison of Granger causal
influences can, in fact, yield an answer that is opposite to the true
direction of information flow.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/publications#Venkatesh2015Direction_Allerton&quot; class=&quot;btn btn--light&quot;&gt;&lt;i class=&quot;fa fa-file-text-o fa-lg&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Link to paper&lt;/a&gt;
&lt;a href=&quot;/publications#Venkatesh2015Direction_SfN&quot; class=&quot;btn btn--light&quot;&gt;&lt;i class=&quot;fa fa-file-text-o fa-lg&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt; Link to poster&lt;/a&gt;&lt;/p&gt;</content><author><name>Praveen Venkatesh</name></author><category term="research" /><category term="Granger Causality" /><category term="Directed Information" /><category term="Information flow" /><category term="Message" /><summary type="html">Granger causality is an established measure of the “causal influence” that one statistical process has on another. It has been used extensively in neuroscience to infer statistical causal influences. Recently, however, many works in the neuroscience literature have begun to compare Granger causal influences along forward and reverse links of a feedback network in order to determine the direction of information flow in this network.</summary></entry><entry><title type="html">Link for Python logging</title><link href="https://praveenv253.github.io/logs/2017/06/14/log-message-1.html" rel="alternate" type="text/html" title="Link for Python logging" /><published>2017-06-14T23:40:45-07:00</published><updated>2017-06-14T23:40:45-07:00</updated><id>https://praveenv253.github.io/logs/2017/06/14/log-message-1</id><content type="html" xml:base="https://praveenv253.github.io/logs/2017/06/14/log-message-1.html">&lt;p&gt;Note to self: when you get time, figure out how to do logging, as a general practice: &lt;a href=&quot;https://fangpenlin.com/posts/2012/08/26/good-logging-practice-in-python/&quot;&gt;here&lt;/a&gt; is a nice link to get you started.&lt;/p&gt;</content><author><name>Praveen Venkatesh</name></author><category term="logs" /><category term="log message" /><category term="python" /><category term="logging" /><summary type="html">Note to self: when you get time, figure out how to do logging, as a general practice: here is a nice link to get you started.</summary></entry><entry><title type="html">Log Message</title><link href="https://praveenv253.github.io/logs/2017/06/14/log-message-3.html" rel="alternate" type="text/html" title="Log Message" /><published>2017-06-14T23:40:45-07:00</published><updated>2017-06-14T23:40:45-07:00</updated><id>https://praveenv253.github.io/logs/2017/06/14/log-message-3</id><content type="html" xml:base="https://praveenv253.github.io/logs/2017/06/14/log-message-3.html">&lt;p&gt;Note to self: when you get time, figure out how to do logging, as a general practice: &lt;a href=&quot;https://fangpenlin.com/posts/2012/08/26/good-logging-practice-in-python/&quot;&gt;here&lt;/a&gt; is a nice link to get you started.&lt;/p&gt;</content><author><name>Praveen Venkatesh</name></author><category term="logs" /><category term="log message" /><category term="python" /><category term="logging" /><summary type="html">Note to self: when you get time, figure out how to do logging, as a general practice: here is a nice link to get you started.</summary></entry><entry><title type="html">Debugging python deadlocks</title><link href="https://praveenv253.github.io/logs/2017/06/14/log-message-2.html" rel="alternate" type="text/html" title="Debugging python deadlocks" /><published>2017-06-14T23:38:01-07:00</published><updated>2017-06-14T23:38:01-07:00</updated><id>https://praveenv253.github.io/logs/2017/06/14/log-message-2</id><content type="html" xml:base="https://praveenv253.github.io/logs/2017/06/14/log-message-2.html">&lt;p&gt;Sometimes, a python script just gives up and hangs. Even Ctrl-C does not exit the program. You do not have to have a loop with a catch-all &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;except&lt;/code&gt; command or anything. Apparently, this can happen because of what is known as a “deadlock”, which refers to some kind of race condition when running a threaded program. I was not running a threaded program, except I &lt;em&gt;was&lt;/em&gt;, because I was using BLAS. In this case, OpenBLAS, which appears to have had a history with deadlocks, and maybe still does, or maybe it was due to my having accidentally installed numpy with MKL and scipy with OpenBLAS. In any event, the only reason I found out that the issue was with blas.py is thanks to &lt;a href=&quot;https://pypi.python.org/pypi/hanging_threads/2.0.3&quot;&gt;this excellent little piece of software&lt;/a&gt;, which shows you a stack trace if your program is inactive for more than 10 seconds. It is a python package called &lt;a href=&quot;https://github.com/niccokunzmann/hanging_threads&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hanging_threads&lt;/code&gt;&lt;/a&gt; and works like a charm. This invaluable tool was found after I finally figured out what to google for, and landed on &lt;a href=&quot;https://stackoverflow.com/q/3443607/525169&quot;&gt;this SO page&lt;/a&gt;, and then scrolled down a lot after the top answer (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;trace&lt;/code&gt;) did not work for me. &lt;a href=&quot;https://docs.python.org/3.4/library/trace.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;trace&lt;/code&gt;&lt;/a&gt; is a python module and is also really useful. It prints out the commands currently being executed. This is really neat because if your program suddenly (and hopefully deterministically) gives up, you get a full history of all the commands executed. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--ignore-dir&lt;/code&gt; flag is extremely useful, since you can avoid printing out various internal modules. I was lucky, since my packages of interest (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sht&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cross_validation&lt;/code&gt;, etc.) were under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$HOME/.local&lt;/code&gt;, so I could just supply &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--ignore-dir=/usr&lt;/code&gt; and prevent tracing through imports like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sys&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numpy&lt;/code&gt;.&lt;/p&gt;</content><author><name>Praveen Venkatesh</name></author><category term="logs" /><category term="log message" /><category term="python" /><category term="deadlock" /><category term="hanging" /><category term="stuck" /><category term="trace" /><summary type="html">Sometimes, a python script just gives up and hangs. Even Ctrl-C does not exit the program. You do not have to have a loop with a catch-all except command or anything. Apparently, this can happen because of what is known as a “deadlock”, which refers to some kind of race condition when running a threaded program. I was not running a threaded program, except I was, because I was using BLAS. In this case, OpenBLAS, which appears to have had a history with deadlocks, and maybe still does, or maybe it was due to my having accidentally installed numpy with MKL and scipy with OpenBLAS. In any event, the only reason I found out that the issue was with blas.py is thanks to this excellent little piece of software, which shows you a stack trace if your program is inactive for more than 10 seconds. It is a python package called hanging_threads and works like a charm. This invaluable tool was found after I finally figured out what to google for, and landed on this SO page, and then scrolled down a lot after the top answer (trace) did not work for me. trace is a python module and is also really useful. It prints out the commands currently being executed. This is really neat because if your program suddenly (and hopefully deterministically) gives up, you get a full history of all the commands executed. The --ignore-dir flag is extremely useful, since you can avoid printing out various internal modules. I was lucky, since my packages of interest (sht, cross_validation, etc.) were under $HOME/.local, so I could just supply --ignore-dir=/usr and prevent tracing through imports like sys and numpy.</summary></entry><entry><title type="html">Visualizing the output of python’s cprofile</title><link href="https://praveenv253.github.io/logs/2017/06/13/log-message-1.html" rel="alternate" type="text/html" title="Visualizing the output of python’s cprofile" /><published>2017-06-13T01:21:16-07:00</published><updated>2017-06-13T01:21:16-07:00</updated><id>https://praveenv253.github.io/logs/2017/06/13/log-message-1</id><content type="html" xml:base="https://praveenv253.github.io/logs/2017/06/13/log-message-1.html">&lt;p&gt;To better visualize the call tree after profiling a python script, first save your output from cProfile using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python -m cProfile -o &amp;lt;myscript.profile&amp;gt; &amp;lt;myscript.py&amp;gt;&lt;/code&gt;. You will need to install &lt;a href=&quot;https://pypi.python.org/pypi/pyprof2calltree/&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pyprof2calltree&lt;/code&gt;&lt;/a&gt; via pip and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kcachegrind&lt;/code&gt; via apt. Then, visualize the profile data with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pyprof2calltree -i &amp;lt;myscript.profile&amp;gt; -k&lt;/code&gt;. Reference: &lt;a href=&quot;https://stackoverflow.com/a/3561512/525169&quot;&gt;this&lt;/a&gt; SO answer.&lt;/p&gt;</content><author><name>Praveen Venkatesh</name></author><category term="logs" /><category term="log message" /><category term="python" /><category term="profiling" /><category term="visualization" /><category term="kcachegrind" /><summary type="html">To better visualize the call tree after profiling a python script, first save your output from cProfile using python -m cProfile -o &amp;lt;myscript.profile&amp;gt; &amp;lt;myscript.py&amp;gt;. You will need to install pyprof2calltree via pip and kcachegrind via apt. Then, visualize the profile data with pyprof2calltree -i &amp;lt;myscript.profile&amp;gt; -k. Reference: this SO answer.</summary></entry><entry><title type="html">Export syntax-highlighted pdf files from vim</title><link href="https://praveenv253.github.io/logs/2016/09/13/log-message.html" rel="alternate" type="text/html" title="Export syntax-highlighted pdf files from vim" /><published>2016-09-13T11:07:40-07:00</published><updated>2016-09-13T11:07:40-07:00</updated><id>https://praveenv253.github.io/logs/2016/09/13/log-message</id><content type="html" xml:base="https://praveenv253.github.io/logs/2016/09/13/log-message.html">&lt;p&gt;If you want to export from vim to a pdf file with syntax highlighting, hardcopy does not seem to use the correct syntax highlighting file. Or rather, it uses GUI settings, which means that unless your syntax highlighting file takes care of looking good on gvim, it will not give you a good result. For you to get syntax highlighting &lt;em&gt;at all&lt;/em&gt;, you must first &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:set term=xterm-256color-italic&lt;/code&gt; even if it is already set. Do it even if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:set term&lt;/code&gt; returns &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xterm-256color-italic&lt;/code&gt;. Then, to actually get &lt;em&gt;terminal colours&lt;/em&gt;, use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:TOhtml&lt;/code&gt;, which converts the file into an html page, which you can then print from your browser. For a good print colorscheme, run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:colorscheme print&lt;/code&gt;, which is is a custom light-background colorscheme I made for printing. Do this before running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:TOhtml&lt;/code&gt;, obviously.&lt;/p&gt;</content><author><name>Praveen Venkatesh</name></author><category term="logs" /><category term="log message" /><category term="vim" /><category term="print" /><category term="hardcopy" /><category term="syntax-highlighting" /><category term="colorscheme" /><summary type="html">If you want to export from vim to a pdf file with syntax highlighting, hardcopy does not seem to use the correct syntax highlighting file. Or rather, it uses GUI settings, which means that unless your syntax highlighting file takes care of looking good on gvim, it will not give you a good result. For you to get syntax highlighting at all, you must first :set term=xterm-256color-italic even if it is already set. Do it even if :set term returns xterm-256color-italic. Then, to actually get terminal colours, use :TOhtml, which converts the file into an html page, which you can then print from your browser. For a good print colorscheme, run :colorscheme print, which is is a custom light-background colorscheme I made for printing. Do this before running :TOhtml, obviously.</summary></entry><entry><title type="html">Making vim display italicised text</title><link href="https://praveenv253.github.io/logs/2016/09/12/log-message.html" rel="alternate" type="text/html" title="Making vim display italicised text" /><published>2016-09-12T15:23:07-07:00</published><updated>2016-09-12T15:23:07-07:00</updated><id>https://praveenv253.github.io/logs/2016/09/12/log-message</id><content type="html" xml:base="https://praveenv253.github.io/logs/2016/09/12/log-message.html">&lt;p&gt;To make vim display italicised text, see &lt;a href=&quot;http://www.nerdyweekly.com/posts/enable-italic-text-vim-tmux-gnome-terminal/&quot;&gt;this&lt;/a&gt; wonderful blog entry. Essentially, we need to manually map the escape sequences to the keywords &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sitm&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ritm&lt;/code&gt;, which are used by vim to set and remove italic mode respectively. The file in question, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xterm-256color-italic.terminfo&lt;/code&gt;, has been added to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;misc&lt;/code&gt; repository as well. It contains all relevant install info. Note that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TERM&lt;/code&gt; must be set to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xterm-256color-italic&lt;/code&gt; by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gnome-terminal&lt;/code&gt; for this to take effect. &lt;a href=&quot;http://askubuntu.com/questions/233280/gnome-terminal-reports-term-to-be-xterm&quot;&gt;This&lt;/a&gt; askubuntu question tells you how to do it. Set the command executed by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gnome-terminal&lt;/code&gt; to be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;env TERM=xterm-256color-italic /bin/bash&lt;/code&gt;.&lt;/p&gt;</content><author><name>Praveen Venkatesh</name></author><category term="logs" /><category term="log message" /><category term="vim" /><category term="italics" /><category term="comments" /><summary type="html">To make vim display italicised text, see this wonderful blog entry. Essentially, we need to manually map the escape sequences to the keywords sitm and ritm, which are used by vim to set and remove italic mode respectively. The file in question, xterm-256color-italic.terminfo, has been added to the misc repository as well. It contains all relevant install info. Note that TERM must be set to xterm-256color-italic by gnome-terminal for this to take effect. This askubuntu question tells you how to do it. Set the command executed by gnome-terminal to be env TERM=xterm-256color-italic /bin/bash.</summary></entry></feed>